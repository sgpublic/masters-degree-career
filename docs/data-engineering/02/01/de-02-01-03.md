# 2.1.3 常用机器学习算法介绍

## 2.1.3.1 线性回归

**线性回归（Linear Regression）** 使用**权重参数**和**偏执参数**来拟合目标输出。

假设一个样例 $x=(x_1,\ x_2,\ ...,\ x_d)$，即一共拥有 $d$ 个特征，第 $i$ 个特征的值为 $x_i$，那么设权重参数为 $\omega=(\omega_1,\ \omega_2,\ ...,\ \omega_d)$，偏执参数为 $b$，则模型输出结果为：

$$
f(x)=\omega_{1}x_{1}+\omega_{2}x_{2}+...\omega_{d}x_{d}+b
$$

从几何上来说，线性回归是在 $n$ 维空间（即 $n$ 个特征）中用一个 $n-1$ 维的子空间来尽量拟合目标输出。

线性回归适用于回归任务，因此我们可以尝试使均方误差 MSE 最小化来优化模型参数，即：

$$
(\omega^*,b^*)=argmin_{(\omega,b)}\sum_{i=1}^{m}(f(x_i)-y_i)^2
$$

> 注：$argmin$ 指取极小值点。

<details>
<summary>PyTorch</summary>

```python
torch.nn.MSELoss()
```

</details>

## 2.1.3.2 决策树

**决策树（Decision Tree）** 使用多个标签尽力将数据集分为不同的类别。

+ CERT 决策树

   假设一个数据集 $D$ 中，属于 $i$ 类的样例占整个数据集的比例为 $p_i$（$i=1,2,...,k$），则**基尼指数 Gini**（数据集 D 的**纯度**）为：

   $$
   Gini(D)=\sum_{i=1}^{k}p_i(1-p_i)=1-\sum_{i=1}^{k}p_i^2
   $$

   基尼指数反映了从数据集 D 中随机抽取两个样例，其类别不一致的概率。因此 Gini 越小，数据集 D 的纯度越高。

   于是在构建树的时候，可使用尼基指数最小的特征作为当前层的划分依据。

   由于特征值一般是连续的数值，要实现分类，CERT 决策树会尝试在每个样本的位置将数据集切分为两个部分，并计算基尼指数，取最小值的位置就行切分。

   <details>
   <summary>scikit-learn</summary>

   ```python
   sklearn.tree.DecisionTreeClassifier()
   ```

   </details>

+ Histogram 决策树

   Histogram 决策树是一种基于 CERT 决策树的优化版本，它将连续特征离散化，然后使用 CERT 决策树进行分类。

   具体的，由于原始 CERT 会在每个位置尝试切分，这种方式在面对大量数据时会显著降低计算速度，因此 Histogram 将数据集分为多个区间，仅在区间边界处尝试切分，从而减少计算量。


## 2.1.3.3 集成学习

**集成学习（Ensemble Learning）** 是一种通过构建多个模型来提高模型性能的方法。

通过独立训练多个模型，然后将它们的预测结果进行组合（例如计算平均、投票等方式），从而得到更好的预测结果。

## 2.1.3.4 随机森林

**随机森林（Random Forest）** 在基于决策树的集成学习的基础上，引入了两个随机：

1. 每个数使用的数据集是从原数据集中随机选取的。
2. 每个决策树使用随机选取的特征寻找最佳切分点，而不是使用基尼指数最低的特征来切分。

## 2.1.3.5 XGBoost

XGBoost 是基于 CART 的集成学习方法。 XGBoost 使用了很多方法来提高训练效率，比如：

+ 正则化：即在损失函数后面加一个约束项（比如 L2 正则化是加上所有参数的平方和），从而防止过拟合。
+ 二阶泰勒展开：用于近似损失函数，这样做可以减少计算量。
+ 剪枝：当一次分裂让模型表现提升幅度（即 Gain）过小的时候，回退此次分裂。

## 2.1.3.6 LightGBM

LightGBM 是基于 Histogram 决策树的集成学习方法。由于 Histogram 的特性，LightGBM 计算速度比 XGBoost 更快，内存占用也更小。
